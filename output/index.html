<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Python Testing Tools</title><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.com/hovercraft"></meta><meta name="author" content="Carl Meyer"></meta><meta name="description" content="a presentation for ConFoo 2014"></meta><meta name="keywords" content="presentation, python, testing, confoo"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/impressConsole.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="css/oddbird.css" media="all"></link></head><body class="impress-not-supported"><div id="impress" data-transition-duration="400"><div class="step" step="0" id="title" data-x="0" data-y="0"><h1 id="python-testing-tools">Python Testing Tools</h1><p><div class="vcard">
<a href="http://www.oddbird.net">
  <img src="images/logo.svg" alt="OddBird" class="logo" />
</a>
<h2 class="fn">Carl Meyer</h2>
<ul class="links">
  <li><a href="http://www.oddbird.net" class="org url">oddbird.net</a></li>
  <li><a href="https://twitter.com/carljm" rel="me">@carljm</a></li>
</ul>
</div></p></div><div class="step" step="1" id="thistalk" data-reveal="1" data-x="1600" data-y="0"><h1 id="this-talk">This talk</h1><ul><li>py.test</li><li>test marking &amp; skipping</li><li>parametrized tests</li><li>fixtures</li><li>coverage</li><li>tox</li><li>mock &amp; pretend</li><li>WebTest</li></ul></div><div class="step" step="2" data-reveal="1" data-x="3200" data-y="0"><h1 id="how">How</h1><ul><li>Introduce a feature</li><li>Example(s)</li><li>Further exploration</li><li>Link to docs</li><li>Python 3</li></ul><div class="notes"><p>Docs links on final slide. No stress; doc links will be live in online
slides.</p><p>All code is Py3, but will note Py2 differences.</p></div></div><div class="step" step="3" data-reveal="1" data-x="4800" data-y="0"><h1 id="me">Me</h1><ul><li>Writing Python since 2002.</li><li>Professionally since 2007.</li><li>Mostly web development.</li><li>OSS: pip, virtualenv, Django</li></ul></div><div class="step" step="4" data-emphasize-lines-step="1,3,5,7,9,10" data-x="6400" data-y="0"><h1 id="py-test">py.test</h1><h2 id="grades-py">grades.py</h2><pre class="highlight code python"><span class="ln">1 </span><span class="k">def</span> <span class="nf">get_level</span><span class="p">(</span><span class="n">min_grade</span><span class="p">,</span> <span class="n">max_grade</span><span class="p">):</span>
<span class="ln">2 </span>    <span class="k">if</span> <span class="n">max_grade</span> <span class="o">&lt;=</span> <span class="mi">6</span><span class="p">:</span>
<span class="ln">3 </span>        <span class="k">return</span> <span class="s">'elementary'</span>
<span class="ln">4 </span>    <span class="k">if</span> <span class="n">min_grade</span> <span class="o">&gt;</span> <span class="mi">6</span><span class="p">:</span>
<span class="ln">5 </span>        <span class="k">return</span> <span class="s">'secondary'</span>
<span class="ln">6 </span>    <span class="k">return</span> <span class="bp">None</span></pre><h2 id="test-grades-py">test_grades.py</h2><pre class="highlight code python"><span class="ln">1 </span><span class="kn">import</span> <span class="nn">grades</span>
<span class="ln">2 </span>
<span class="ln">3 </span><span class="k">def</span> <span class="nf">test_get_level</span><span class="p">():</span>
<span class="ln">4 </span>    <span class="k">assert</span> <span class="n">grades</span><span class="o">.</span><span class="n">get_level</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="s">'elementary'</span></pre><div class="notes"><p>py.test will find and run tests in any file whose name begins with
"test_". Test functions also need to have names beginning with "test".</p><p>Matching up "grades.py" with "test_grades.py" is not necessary, though often
helpful to keep tests organized.</p><p>(These naming conventions are customizable.)</p><p>Low-boilerplate tests: plain functions, plain asserts.</p></div></div><div class="step" step="5" data-emphasize-lines-step="1,2,4,5,7,9" data-x="8000" data-y="0"><pre class="highlight code"><span class="ln">1 </span>$ pip install pytest</pre><pre class="highlight code"><span class="ln">1 </span>$ py.test
<span class="ln">2 </span>============== test session starts ========================
<span class="ln">3 </span>platform linux -- Python 3.3.2 -- py-1.4.20 -- pytest-2.5.2
<span class="ln">4 </span>collected 1 items
<span class="ln">5 </span>
<span class="ln">6 </span>test_grades.py .
<span class="ln">7 </span>
<span class="ln">8 </span>============== 1 passed in 0.01 seconds ===================</pre><div class="notes"><p>To run the tests, just "pip install pytest" and run "py.test" - it will
automatically find and run your tests. Here it runs our one test, which
passes!</p></div></div><div class="step" step="6" data-pytest-highlight="1" data-emphasize-lines-step="6,12,13,14,15" data-x="9600" data-y="0"><h1 id="helpful-failures">Helpful failures</h1><pre class="highlight code"><span class="ln"> 1 </span>$ py.test
<span class="ln"> 2 </span>================ test session starts ======================
<span class="ln"> 3 </span>platform linux -- Python 3.3.2 -- py-1.4.20 -- pytest-2.5.2
<span class="ln"> 4 </span>collected 1 items
<span class="ln"> 5 </span>
<span class="ln"> 6 </span>test_grades.py F
<span class="ln"> 7 </span>
<span class="ln"> 8 </span>================ FAILURES ==================================
<span class="ln"> 9 </span>________________ test_get_level ____________________________
<span class="ln">10 </span>
<span class="ln">11 </span>    def test_get_level():
<span class="ln">12 </span>&gt;       assert grades.get_level(2, 5) == 'secondary'
<span class="ln">13 </span>E       assert 'elementary' == 'secondary'
<span class="ln">14 </span>E         - elementary
<span class="ln">15 </span>E         + secondary
<span class="ln">16 </span>
<span class="ln">17 </span>test_grades.py:4: AssertionError
<span class="ln">18 </span>================ 1 failed in 0.02 seconds ==================</pre></div><div class="step" step="7" data-reveal="1" data-x="11200" data-y="0"><h1 id="python-test-runners">Python test runners</h1><h2 id="a-brief-synopsis-and-digression">A brief synopsis and digression</h2><ul><li>We saw <a href="http://pytest.org">py.test</a> in action: <tt>pip install pytest; py.test</tt></li><li><a href="https://nose.readthedocs.org/">Nose</a> is similar: <tt>pip install nose; nosetests</tt></li><li>Both can run simple function tests with asserts.</li><li><a href="http://docs.python.org/3.3/library/unittest.html">unittest</a> is in the standard library, similar to "xUnit" test frameworks in
various languages. Tests require a bit more boilerplate. <tt>python -m unittest
discover</tt></li><li>Others: <a href="http://twistedmatrix.com/trac/wiki/TwistedTrial">twisted.trial</a>, <a href="https://pypi.python.org/pypi/zope.testrunner">zope.testrunner</a></li></ul><div class="notes"><p>If all these choices are overwhelming, don't worry about it. They're all
fine, just pick one and run with it.</p><p>My choice is py.test, so that's what I'll be covering today.</p></div></div><div class="step" step="8" data-reveal="1" data-x="12800" data-y="0"><h1 id="choosing-tests-to-run">Choosing tests to run</h1><ul><li>Name a test file: <tt>py.test path/to/test_grades.py</tt></li><li>Name a directory: <tt>py.test some/tests/</tt></li><li>Match test function/class name: <tt>py.test -k grades</tt></li><li>Select tests by "mark": <tt>py.test -m "not slow"</tt></li></ul><div class="notes"><p>Flexible matching of tests to run - very important for fast edit/test
cycles, especially in larger projects.</p><p>Select by mark - which raises the question...</p></div></div><div class="step" step="9" data-emphasize-lines-step="3" data-reveal="1" data-x="14400" data-y="0"><h1 id="wait-what-s-a-mark">Wait, what's a "mark"?</h1><pre class="highlight code python"><span class="ln">1 </span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="ln">2 </span>
<span class="ln">3 </span><span class="nd">@pytest.mark.slow</span>
<span class="ln">4 </span><span class="k">def</span> <span class="nf">test_something_very_slow</span><span class="p">():</span>
<span class="ln">5 </span>    <span class="sd">"""Can download the internet."""</span>
<span class="ln">6 </span>    <span class="c"># ...</span></pre><ul><li><tt>py.test -m slow</tt> will run only tests with this mark.</li><li><tt>py.test -m "not slow"</tt> will run only tests without it.</li><li>Can also use <tt>and</tt> / <tt>or</tt> for conditions with multiple marks.</li></ul><div class="notes"><p>You can use any mark names you want (valid Python identifiers) or configure
a restricted set for the project in your <tt>pytest.ini</tt> file.</p></div></div><div class="step" step="10" data-emphasize-lines-step="2,3,4,5,6,7,8,9" data-x="16000" data-y="0"><h1 id="pytest-ini">pytest.ini</h1><pre class="highlight code ini"><span class="ln"> 1 </span><span class="k">[pytest]</span>
<span class="ln"> 2 </span><span class="na">minversion</span> <span class="o">=</span> <span class="s">2.4.2</span>
<span class="ln"> 3 </span><span class="na">addopts</span> <span class="o">=</span> <span class="s">--strict --cov-report html --cov myproj</span>
<span class="ln"> 4 </span><span class="na">norecursedirs</span> <span class="o">=</span> <span class="s">.* _* selenium node_modules qunit</span>
<span class="ln"> 5 </span><span class="na">python_files</span> <span class="o">=</span> <span class="s">test_*.py</span>
<span class="ln"> 6 </span><span class="na">python_classes</span> <span class="o">=</span> <span class="s">Test</span>
<span class="ln"> 7 </span><span class="na">python_functions</span> <span class="o">=</span> <span class="s">test</span>
<span class="ln"> 8 </span><span class="na">markers</span> <span class="o">=</span><span class="s">
</span><span class="ln"> 9 </span><span class="s">    slow: mark a test as slow
</span><span class="ln">10 </span><span class="s">    web: mark a test as a web test</span></pre><div class="notes"><p>All optional.</p><p>If you use markers, recommended to list valid markers so there's one
reference point for all markers used, and typos become errors (with
<tt>strict</tt>). At some point in the future pytest may require markers to be
registered.</p></div></div><div class="step" step="11" data-reveal="1" data-x="17600" data-y="0"><h1 id="classes">Classes?</h1><pre class="highlight code python"><span class="k">class</span> <span class="nc">TestPager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">test_num_pages</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Can calculate total number of pages."""</span>
        <span class="k">assert</span> <span class="n">pager</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="mi">23</span><span class="p">,</span> <span class="n">pagesize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">num_pages</span> <span class="o">==</span> <span class="mi">3</span>

    <span class="k">def</span> <span class="nf">test_item_range</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Can calculate range of items to be shown."""</span>
        <span class="c"># ...</span></pre><ul><li>Can use classes to group related tests, but not required.</li><li>Unlike <tt>unittest</tt>, no special <tt>TestCase</tt> class to inherit from.</li><li>Avoid using classes for setup/teardown (use fixtures).</li><li>Avoid using classes to parametrize tests (use parametrized tests).</li></ul></div><div class="step" step="12" data-emphasize-lines-step="4,5" data-x="19200" data-y="0"><h1 id="test-skipping">Test skipping</h1><h2 id="not-all-tests-can-run-in-all-environments">Not all tests can run in all environments.</h2><pre class="highlight code python"><span class="ln">1 </span><span class="kn">import</span> <span class="nn">sys</span>
<span class="ln">2 </span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="ln">3 </span>
<span class="ln">4 </span><span class="nd">@pytest.mark.skipif</span><span class="p">(</span>
<span class="ln">5 </span>    <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">!=</span> <span class="s">'win32'</span><span class="p">,</span> <span class="n">reason</span><span class="o">=</span><span class="s">'Windows specific'</span><span class="p">)</span>
<span class="ln">6 </span><span class="k">def</span> <span class="nf">test_updates_registry</span><span class="p">():</span>
<span class="ln">7 </span>    <span class="sd">"""Checks and updates registry entries."""</span>
<span class="ln">8 </span>    <span class="c"># ...</span></pre><div class="notes"><p>Can mark any test to be skipped under some conditions.</p></div></div><div class="step" step="13" data-emphasize-lines-step="6,9,16" data-x="20800" data-y="0"><pre class="highlight code"><span class="ln">1 </span>$ py.test
<span class="ln">2 </span>================ test session starts ======================
<span class="ln">3 </span>platform linux -- Python 3.3.2 -- py-1.4.20 -- pytest-2.5.2
<span class="ln">4 </span>collected 4 items
<span class="ln">5 </span>
<span class="ln">6 </span>test_grades.py ...s
<span class="ln">7 </span>
<span class="ln">8 </span>================ 3 passed, 1 skipped in 0.02 seconds ======</pre><pre class="highlight code"><span class="ln"> 1 </span>$ py.test -rs
<span class="ln"> 2 </span>================ test session starts ======================
<span class="ln"> 3 </span>platform linux -- Python 3.3.2 -- py-1.4.20 -- pytest-2.5.2
<span class="ln"> 4 </span>collected 4 items
<span class="ln"> 5 </span>
<span class="ln"> 6 </span>test_grades.py ...s
<span class="ln"> 7 </span>================ short test summary info ==================
<span class="ln"> 8 </span>SKIP [1] /.../_pytest/skipping.py:132: Windows specific
<span class="ln"> 9 </span>
<span class="ln">10 </span>================ 3 passed, 1 skipped in 0.02 seconds ======</pre><div class="notes"><p>Skipped tests show up as an 's' instead of a '.'.</p><p>Run py.test with '-rs' to show reasons for skipped tests.</p></div></div><div class="step" step="14" data-emphasize-lines-step="4,5" data-reveal="1" data-x="22400" data-y="0"><h1 id="expected-failures">Expected failures</h1><h2 id="sometimes-we-expect-a-test-to-fail-for-now">Sometimes we expect a test to fail, for now.</h2><pre class="highlight code python"><span class="ln">1 </span><span class="kn">import</span> <span class="nn">sys</span>
<span class="ln">2 </span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="ln">3 </span>
<span class="ln">4 </span><span class="nd">@pytest.mark.xfail</span><span class="p">(</span>
<span class="ln">5 </span>    <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">reason</span><span class="o">=</span><span class="s">"Buggy on Py 3.4"</span><span class="p">)</span>
<span class="ln">6 </span><span class="k">def</span> <span class="nf">test_something_that_doesnt_work_yet_on_python_34</span><span class="p">():</span>
<span class="ln">7 </span>    <span class="k">pass</span> <span class="c"># ...</span></pre><ul><li>Just like <tt>-rs</tt> for skips, <tt>-rx</tt> will provide additional info on expected
failures.</li><li><tt>xfail</tt> tests will report an <tt>X</tt> (<tt>xpass</tt> or "unexpected pass") if they
pass.</li><li>Use <tt>--runxfail</tt> to run <tt>xfail</tt> tests normally (report failures as
failures).</li></ul><div class="notes"><p>May be a low priority bug that we plan to fix, or a feature we haven't fully
implemented yet.</p><p>Can also unconditionally xfail, provide only a reason.</p></div></div><div class="step" step="15" data-reveal="1" data-x="24000" data-y="0"><h1 id="marking-classes">Marking classes</h1><pre class="highlight code python"><span class="kn">import</span> <span class="nn">pytest</span>

<span class="nd">@pytest.mark.xfail</span>
<span class="k">class</span> <span class="nc">TestPager</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">test_num_pages</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ...</span>

    <span class="k">def</span> <span class="nf">test_item_range</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c"># ...</span></pre><ul><li>Can apply a mark to an entire test class.</li><li>Equivalent to applying it to each individual test method.</li></ul></div><div class="step" step="16" data-reveal="1" data-x="25600" data-y="0"><h1 id="parametrized-tests">Parametrized tests</h1><ul><li>Running a set of similar tests with an array of different inputs and outputs.</li><li>Running the same test multiple times under different
configurations/conditions.</li></ul></div><div class="step" step="17" data-emphasize-lines-step="2,3,4,8,9" data-reveal="1" data-x="27200" data-y="0"><h1 id="naive-approach">Naive approach</h1><pre class="highlight code python"><span class="ln"> 1 </span><span class="k">def</span> <span class="nf">test_sum</span><span class="p">():</span>
<span class="ln"> 2 </span>    <span class="n">tests</span> <span class="o">=</span> <span class="p">[</span>
<span class="ln"> 3 </span>        <span class="p">([],</span> <span class="mi">0</span><span class="p">),</span>
<span class="ln"> 4 </span>        <span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">3</span><span class="p">),</span>
<span class="ln"> 5 </span>        <span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span>
<span class="ln"> 6 </span>        <span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span>
<span class="ln"> 7 </span>        <span class="p">]</span>
<span class="ln"> 8 </span>    <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">tests</span><span class="p">:</span>
<span class="ln"> 9 </span>        <span class="k">assert</span> <span class="nb">sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="n">output</span></pre><ul><li>Accomplishes the goal, but...</li><li>Early failure short-circuits (don't know which others would have failed).</li><li>For more complex cases, don't get e.g. separate setup/teardown.</li><li>Ideally these would each be treated as a separate test.</li></ul></div><div class="step" step="18" data-emphasize-lines-step="3,4,5,6,7,8,9,10" data-x="28800" data-y="0"><h1 id="test-sum-py">test_sum.py</h1><pre class="highlight code python"><span class="ln"> 1 </span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="ln"> 2 </span>
<span class="ln"> 3 </span><span class="nd">@pytest.mark.parametrize</span><span class="p">(</span>
<span class="ln"> 4 </span>    <span class="s">'inputs,output'</span><span class="p">,</span>
<span class="ln"> 5 </span>    <span class="p">[</span>   <span class="p">([],</span> <span class="mi">0</span><span class="p">),</span>
<span class="ln"> 6 </span>        <span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">3</span><span class="p">),</span>
<span class="ln"> 7 </span>        <span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">),</span>
<span class="ln"> 8 </span>        <span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>  <span class="p">])</span>
<span class="ln"> 9 </span><span class="k">def</span> <span class="nf">test_sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
<span class="ln">10 </span>    <span class="k">assert</span> <span class="nb">sum</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span> <span class="o">==</span> <span class="n">output</span></pre></div><div class="step" step="19" data-emphasize-lines-step="1,4,6,10,20,21" data-pytest-highlight="1" data-x="30400" data-y="0"><pre class="highlight code"><span class="ln"> 1 </span>$ py.test test_sum.py
<span class="ln"> 2 </span>=============== test session starts =======================
<span class="ln"> 3 </span>platform linux -- Python 3.3.2 -- py-1.4.20 -- pytest-2.5.2
<span class="ln"> 4 </span>collected 4 items
<span class="ln"> 5 </span>
<span class="ln"> 6 </span>test_sum.py ...F
<span class="ln"> 7 </span>
<span class="ln"> 8 </span>================ FAILURES =================================
<span class="ln"> 9 </span>________________ test_sum[inputs3-2] ______________________
<span class="ln">10 </span>inputs = [-4, 3, 2], output = 2
<span class="ln">11 </span>
<span class="ln">12 </span>    @pytest.mark.parametrize(
<span class="ln">13 </span>        'inputs,output',
<span class="ln">14 </span>        [   ([], 0),
<span class="ln">15 </span>            ([1, 2], 3),
<span class="ln">16 </span>            ([0, 2], 2),
<span class="ln">17 </span>            ([-4, 3, 2], 2)  ])
<span class="ln">18 </span>    def test_sum(inputs, output):
<span class="ln">19 </span>&gt;       assert sum(inputs) == output
<span class="ln">20 </span>E       assert 1 == 2
<span class="ln">21 </span>E        +  where 1 = sum([-4, 3, 2])
<span class="ln">22 </span>
<span class="ln">23 </span>test_sum.py:13: AssertionError
<span class="ln">24 </span>================ 1 failed, 3 passed in 0.02 seconds =======</pre></div><div class="step" step="20" data-reveal="1" data-x="32000" data-y="0"><h1 id="py-test-fixtures">py.test fixtures</h1><ul><li>Each test should run in a predictable, repeatable, baseline environment.</li><li>Some tests need resources (a database, the filesystem, an initialized code
object) that may require some <strong>setup</strong> and <strong>teardown</strong> in order to provide
a predictable environment.</li><li>py.test fixtures are a modular system for defining such resources and
allowing tests to request access to them.</li></ul></div><div class="step" step="21" data-emphasize-lines-step="5,7,8,9,12" data-x="33600" data-y="0"><h1 id="example-tempdir">Example: tempdir</h1><pre class="highlight code python"><span class="ln"> 1 </span><span class="kn">import</span> <span class="nn">shutil</span>
<span class="ln"> 2 </span><span class="kn">from</span> <span class="nn">tempfile</span> <span class="kn">import</span> <span class="n">mkdtemp</span>
<span class="ln"> 3 </span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="ln"> 4 </span>
<span class="ln"> 5 </span><span class="nd">@pytest.yield_fixture</span>
<span class="ln"> 6 </span><span class="k">def</span> <span class="nf">tempdir</span><span class="p">():</span>
<span class="ln"> 7 </span>    <span class="n">temp_dir_path</span> <span class="o">=</span> <span class="n">mkdtemp</span><span class="p">()</span>
<span class="ln"> 8 </span>    <span class="k">yield</span> <span class="n">temp_dir_path</span>
<span class="ln"> 9 </span>    <span class="n">shutil</span><span class="o">.</span><span class="n">rmtree</span><span class="p">(</span><span class="n">temp_dir_path</span><span class="p">)</span></pre><pre class="highlight code python"><span class="ln">1 </span><span class="kn">import</span> <span class="nn">os</span>
<span class="ln">2 </span>
<span class="ln">3 </span><span class="k">def</span> <span class="nf">test_write_config</span><span class="p">(</span><span class="n">tempdir</span><span class="p">):</span>
<span class="ln">4 </span>    <span class="sd">"""Writes config to the given file path."""</span>
<span class="ln">5 </span>    <span class="n">config_file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tempdir</span><span class="p">,</span> <span class="s">'test.cfg'</span><span class="p">)</span>
<span class="ln">6 </span>    <span class="c"># ...</span></pre><div class="notes"><p>Py.test actually provides this as a built-in fixture; but it's a nice simple
example, so we'll reimplement it.</p><p>This is a new way to define fixtures, using yield, thus the name of the
decorator.</p><p>A test requests the fixture by asking for an argument of that name, py.test
uses introspection to check the argument names and provide the right fixture
values.</p><p>Each test that requires the fixture will get a new one; the setup and
teardown will be re-executed for every test.</p><p>(The built-in tempdir fixture is a little more complex than this; it leaves
the last few temp dirs laying around to help with debugging failing tests,
and cleans up older ones only.)</p></div></div><div class="step" step="22" data-reveal="1" data-x="35200" data-y="0"><h1 id="fixture-lifecycle-scopes">Fixture lifecycle scopes</h1><ul><li>Default scope is "function": new fixture will be setup and torn down for
each test that requests it.</li><li>Other scopes: "class", "module", "session".</li><li>Setup new fixture once per test class, test module, or test session.</li></ul></div><div class="step" step="23" data-emphasize-lines-step="3,5,6,7,8,9" data-x="36800" data-y="0"><h1 id="session-scope-fixture">Session-scope fixture</h1><pre class="highlight code python"><span class="ln">1 </span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="ln">2 </span>
<span class="ln">3 </span><span class="nd">@pytest.yield_fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s">'session'</span><span class="p">)</span>
<span class="ln">4 </span><span class="k">def</span> <span class="nf">db</span><span class="p">():</span>
<span class="ln">5 </span>    <span class="n">create_test_database</span><span class="p">()</span>
<span class="ln">6 </span>    <span class="n">conn</span> <span class="o">=</span> <span class="n">get_test_database_connection</span><span class="p">()</span>
<span class="ln">7 </span>    <span class="k">yield</span> <span class="n">conn</span>
<span class="ln">8 </span>    <span class="n">destroy_test_database</span><span class="p">()</span></pre><pre class="highlight code python"><span class="ln">1 </span><span class="k">def</span> <span class="nf">test_query</span><span class="p">(</span><span class="n">db</span><span class="p">):</span>
<span class="ln">2 </span>    <span class="k">pass</span> <span class="c"># ...</span></pre><div class="notes"><p>I find few good use cases for class or module-scope fixtures; I'll just give
an example of a session-scope fixture.</p><p>Creating and destroying a database is too slow to do every test; just want
to create the test db once at start of test run and destroy it at the end. A
session-scoped fixture allows this.</p><p>Fixture is lazy: only set up when a test asks for it. So if we run a subset
of our tests that don't ask for the <tt>db</tt> fixture, no test db will be
created for that run - a nice speed boost.</p><p>Problem: database state is not reset between tests. If we add rows in one
test, that could disrupt another test. Violates goal of a repeatable,
predictable environment for each test.</p></div></div><div class="step" step="24" data-emphasize-lines-step="4,11,12,13,14" data-x="38400" data-y="0"><h1 id="paired-fixtures">Paired fixtures</h1><pre class="highlight code python"><span class="ln"> 1 </span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="ln"> 2 </span>
<span class="ln"> 3 </span><span class="nd">@pytest.yield_fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s">'session'</span><span class="p">)</span>
<span class="ln"> 4 </span><span class="k">def</span> <span class="nf">db_conn</span><span class="p">():</span>
<span class="ln"> 5 </span>    <span class="n">create_test_database</span><span class="p">()</span>
<span class="ln"> 6 </span>    <span class="n">conn</span> <span class="o">=</span> <span class="n">get_test_database_connection</span><span class="p">()</span>
<span class="ln"> 7 </span>    <span class="k">yield</span> <span class="n">conn</span>
<span class="ln"> 8 </span>    <span class="n">destroy_test_database</span><span class="p">()</span>
<span class="ln"> 9 </span>
<span class="ln">10 </span><span class="nd">@pytest.yield_fixture</span>
<span class="ln">11 </span><span class="k">def</span> <span class="nf">db</span><span class="p">(</span><span class="n">db_conn</span><span class="p">):</span>
<span class="ln">12 </span>    <span class="k">yield</span> <span class="n">db_conn</span>
<span class="ln">13 </span>    <span class="n">db_conn</span><span class="o">.</span><span class="n">truncate_all_tables</span><span class="p">()</span></pre><pre class="highlight code python"><span class="ln">1 </span><span class="k">def</span> <span class="nf">test_query</span><span class="p">(</span><span class="n">db</span><span class="p">):</span>
<span class="ln">2 </span>    <span class="k">pass</span> <span class="c"># ...</span></pre><div class="notes"><p>Session-scope fixture to create and teardown the test database.</p><p>Function-scope fixture that uses the session-scope fixture and passes it on
to each test, also restoring the database state after each test.</p><p>(Might also be other approaches to restoring state, like running each test
in a transaction and rolling it back.)</p></div></div><div class="step" step="25" data-emphasize-lines-step="1,2,3,4,5,10,11" data-x="40000" data-y="0"><h1 id="parametrized-fixtures">Parametrized fixtures</h1><pre class="highlight code python"><span class="ln"> 1 </span><span class="nd">@pytest.yield_fixture</span><span class="p">(</span>
<span class="ln"> 2 </span>    <span class="n">params</span><span class="o">=</span><span class="p">[</span><span class="s">'sqlite'</span><span class="p">,</span> <span class="s">'mysql'</span><span class="p">,</span> <span class="s">'postgres'</span><span class="p">])</span>
<span class="ln"> 3 </span><span class="k">def</span> <span class="nf">db_conn</span><span class="p">(</span><span class="n">request</span><span class="p">):</span>
<span class="ln"> 4 </span>    <span class="k">if</span> <span class="n">request</span><span class="o">.</span><span class="n">param</span> <span class="o">==</span> <span class="s">'sqlite'</span><span class="p">:</span>
<span class="ln"> 5 </span>        <span class="n">conn</span> <span class="o">=</span> <span class="n">create_sqlite_test_database</span><span class="p">()</span>
<span class="ln"> 6 </span>    <span class="k">elif</span> <span class="n">request</span><span class="o">.</span><span class="n">param</span> <span class="o">==</span> <span class="s">'mysql'</span><span class="p">:</span>
<span class="ln"> 7 </span>        <span class="n">conn</span> <span class="o">=</span> <span class="n">create_mysql_test_database</span><span class="p">()</span>
<span class="ln"> 8 </span>    <span class="k">elif</span> <span class="n">request</span><span class="o">.</span><span class="n">param</span> <span class="o">==</span> <span class="s">'postgres'</span><span class="p">:</span>
<span class="ln"> 9 </span>        <span class="n">conn</span> <span class="o">=</span> <span class="n">create_postgres_test_database</span><span class="p">()</span>
<span class="ln">10 </span>    <span class="k">yield</span> <span class="n">conn</span>
<span class="ln">11 </span>    <span class="n">destroy_test_database</span><span class="p">(</span><span class="n">conn</span><span class="p">)</span></pre><div class="notes"><p>Say we have some tests using a database, and we want to automatically run
all of those tests against all of our supported databases.</p><p>We can take a db fixture like we saw above, and parametrize it.</p><p>Now any test that uses this fixture will run three times, once with each
value for <tt>request.param</tt>.</p><p>Tests that don't use the <tt>db</tt> fixture unaffected.</p></div></div><div class="step" step="26" data-reveal="1" data-x="41600" data-y="0"><h1 id="py-test-plugins">py.test plugins</h1><ul><li><a href="http://pytest.org/latest/xdist.html">pytest-xdist</a></li><li><a href="https://pypi.python.org/pypi/pytest-cov">pytest-cov</a></li><li><a href="https://pypi.python.org/pypi/pytest-bdd">pytest-bdd</a> / <a href="https://pypi.python.org/pypi/pytest-konira">pytest-konira</a></li><li><a href="https://pypi.python.org/pypi/pytest-flakes">pytest-flakes</a></li><li><a href="https://pypi.python.org/pypi/pytest-django">pytest-django</a></li><li><a href="https://pypi.python.org/pypi/pytest-twisted">pytest-twisted</a></li><li><a href="https://pypi.python.org/pypi/pytest-capturelog">pytest-capturelog</a></li><li>...</li></ul></div><div class="step" step="27" data-reveal="1" data-x="43200" data-y="0"><h1 id="py-test-review">py.test review</h1><ul><li>write tests as <strong>simple functions</strong> with <strong>asserts</strong>.</li><li>run the <strong>specific tests</strong> you want.</li><li>get <strong>helpful debugging information</strong> when tests fail.</li><li>mark tests to be <strong>skipped</strong> or as <strong>expected-fails</strong>.</li><li>modular <strong>fixtures</strong> for resources required by tests.</li><li><strong>parametrize</strong> individual tests and fixtures.</li><li>many, many <strong>plugins</strong>.</li></ul></div><div class="step" step="28" data-reveal="1" data-emphasize-lines-step="1,3,5,8" data-x="44800" data-y="0"><h1 id="measuring-test-coverage">Measuring test coverage</h1><p>How much of my production code is exercised by my test suite?</p><pre class="highlight code"><span class="ln">1 </span>$ pip install coverage
<span class="ln">2 </span>
<span class="ln">3 </span>$ coverage run --branch `which py.test`
<span class="ln">4 </span>
<span class="ln">5 </span>$ coverage report --include=grades.py
<span class="ln">6 </span>Name     Stmts   Miss Branch BrMiss  Cover
<span class="ln">7 </span>------------------------------------------
<span class="ln">8 </span>grades       6      3      4      3    40%</pre></div><div class="step" step="29" data-x="46400" data-y="0"><pre class="highlight code">$ coverage html</pre><img src="images/coverage.png" alt="" width="770px" height=""></img><div class="notes"><p>100% coverage not guarantee of adequate tests, but roughly minimum bound.</p></div></div><div class="step" step="30" id="questions" data-x="48000" data-y="0"><h1 id="questions">Questions?</h1><ul><li><a href="http://oddbird.net/python-testing-tools-preso">oddbird.net/python-testing-tools-preso</a></li><li><a href="http://pytest.org">pytest.org</a></li><li><a href="http://nedbatchelder.com/code/coverage/">nedbatchelder.com/code/coverage/</a></li></ul><p><div class="vcard">
<a href="http://www.oddbird.net">
  <img src="images/logo.svg" alt="OddBird" class="logo" />
</a>
<h2 class="fn">Carl Meyer</h2>
<ul class="links">
  <li><a href="http://www.oddbird.net" class="org url">oddbird.net</a></li>
  <li><a href="https://twitter.com/carljm" rel="me">@carljm</a></li>
</ul>
</div></p></div></div><div id="hovercraft-help" class="hide"><table><tr><th>Space</th><td>Forward</td></tr><tr><th>Left, Down, Page Down</th><td>Next slide</td></tr><tr><th>Right, Up, Page Up</th><td>Previous slide</td></tr><tr><th>P</th><td>Open presenter console</td></tr><tr><th>H</th><td>Toggle this help</td></tr></table></div><script type="text/javascript" src="js/jquery-2.1.0.min.js"></script><script type="text/javascript" src="js/oddbird.js"></script><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/impressConsole.js"></script><script type="text/javascript" src="js/hovercraft.js"></script></body></html>